{
  
    
        "post0": {
            "title": "4 - RF model with activations",
            "content": "DATASETS: . (a) Carbonic Anhydrase II (ChEMBL205), a protein lyase, (b) Cyclin-dependent kinase 2 (CHEMBL301), a protein kinase, (c) ether-a-go-go-related gene potassium channel 1 (HERG) (CHEMBL240), a voltage-gated ion channel, (d) Dopamine D4 receptor (CHEMBL219), a monoamine GPCR, (e) Coagulation factor X (CHEMBL244), a serine protease, (f) Cannabinoid CB1 receptor (CHEMBL218), a lipid-like GPCR and (g) Cytochrome P450 19A1 (CHEMBL1978), a cytochrome P450. The activity classes were selected based on data availability and as representatives of therapeutically important target classes or as anti-targets. . In this last notebook we will use the activations and fingerprints created from the previous notebooks and run the same Random Forest classifier on the new dataset. . Create Dataframe from csv file . dataset=&#39;chembl205&#39; . df = pd.read_csv(path/f&#39;{dataset}-data-with-ecfp-activations.csv&#39;) . df.head() . CID SMILES Activity ECFP4_1 ECFP4_2 ECFP4_3 ECFP4_4 ECFP4_5 ECFP4_6 ECFP4_7 ... act_502 act_503 act_504 act_505 act_506 act_507 act_508 act_509 act_510 act_511 . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 98.090538 | -40.475765 | -55.758678 | -40.196342 | -21.565161 | -39.386112 | 88.342987 | -77.520561 | 30.948782 | -48.423111 | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 105.425598 | -53.179356 | -1.957846 | -152.047241 | -124.394402 | -114.585510 | 123.068222 | -48.347355 | 94.419861 | -130.973587 | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 102.208374 | -64.332008 | 33.721348 | -192.948685 | -178.112000 | -143.599503 | 147.062531 | -47.969799 | 126.356018 | -158.886963 | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 105.915039 | -39.527615 | -23.374825 | -81.049446 | -53.409107 | -57.353622 | 98.793694 | -46.956039 | 50.408783 | -67.285126 | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 127.697937 | -41.359352 | -46.951527 | -80.075348 | -57.078045 | -62.758518 | 107.616997 | -67.371101 | 54.477600 | -71.601402 | . 5 rows × 1541 columns . Train test split . train = df.loc[df.is_valid==False] test = df.loc[df.is_valid==True] . Random Forest . def train_rf(X_train, X_test, y_train, y_test, n_estimators=5, criterion=&#39;gini&#39;, max_features=&#39;log2&#39;): rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, min_samples_split=2, max_features=max_features, max_leaf_nodes=None,bootstrap=False,oob_score=False, n_jobs=-1, random_state=69) rf.fit(X_train,y_train) y_pred= rf.predict(X_test) y_pred_prob=rf.predict_proba(X_test) temp=[] for j in range(len(y_pred_prob)): temp.append(y_pred_prob[j][1]) auc=roc_auc_score(np.array(y_test),np.array(temp)) acc2=accuracy_score(y_test,y_pred) mcc=matthews_corrcoef(y_test,y_pred) Recall=recall_score(y_test, y_pred,pos_label=1) Precision=precision_score(y_test, y_pred,pos_label=1) F1_score=f1_score(y_test, y_pred,pos_label=1) return auc,acc2,mcc,Recall,Precision,F1_score . Train RF with settings grid from paper . param_grid = { &#39;n_estimators&#39;: [10,50,100,200,300,700], &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_features&#39;: [&#39;log2&#39;, &#39;sqrt&#39;]} param_grid = ParameterGrid(param_grid) . MCC=[] . for setting in param_grid: print(f&quot;Testing combination: {setting}&quot;) aucs, accs, mccs, recalls, precs, f1_scores = [], [], [], [], [], [] auc,acc2,mcc,recall,precision,F1_score = train_rf(X_train, X_test, y_train, y_test, n_estimators=setting[&#39;n_estimators&#39;], criterion=setting[&#39;criterion&#39;], max_features=setting[&#39;max_features&#39;]) aucs.append(auc) accs.append(acc2) mccs.append(mcc) MCC.append(mcc) recalls.append(recall) precs.append(precision) f1_scores.append(F1_score) print(f&quot;Average Matthews correlation: {np.mean(mccs)}&quot;) print() . Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 10} Average Matthews correlation: 0.8135612528114817 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 50} Average Matthews correlation: 0.808070667802557 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 100} Average Matthews correlation: 0.8057158482003463 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 200} Average Matthews correlation: 0.8057158482003463 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 300} Average Matthews correlation: 0.8033780300912903 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 700} Average Matthews correlation: 0.8057158482003463 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 10} Average Matthews correlation: 0.8085109587994693 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 50} Average Matthews correlation: 0.8084548331984114 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 100} Average Matthews correlation: 0.8108159603070193 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 200} Average Matthews correlation: 0.8088434851265638 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 300} Average Matthews correlation: 0.808070667802557 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 700} Average Matthews correlation: 0.8068911193845247 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 10} Average Matthews correlation: 0.8115762126442005 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 50} Average Matthews correlation: 0.8084548331984114 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 100} Average Matthews correlation: 0.8068911193845247 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 200} Average Matthews correlation: 0.8084548331984114 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 300} Average Matthews correlation: 0.8072806773744206 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 700} Average Matthews correlation: 0.8072806773744206 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 10} Average Matthews correlation: 0.801009124961741 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 50} Average Matthews correlation: 0.8084548331984114 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 100} Average Matthews correlation: 0.8084548331984114 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 200} Average Matthews correlation: 0.8068911193845247 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 300} Average Matthews correlation: 0.8068911193845247 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 700} Average Matthews correlation: 0.8084548331984114 . . The mean score of the Matthews correlation from all the different testing combinations. . np.mean(MCC) . 0.8076599473510552 . np.amax(MCC), top_mcc_scores[&#39;CHEMBL205&#39;] . (0.8135612528114817, 0.862) .",
            "url": "https://drug-discovery.github.io/Drug-Discovery-blog/jupyter/rf/random%20forest/activations/2021/04/24/4.0-RF_model_with_activations.html",
            "relUrl": "/jupyter/rf/random%20forest/activations/2021/04/24/4.0-RF_model_with_activations.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "3 - Create activations",
            "content": "DATASETS: . (a) Carbonic Anhydrase II (ChEMBL205), a protein lyase, (b) Cyclin-dependent kinase 2 (CHEMBL301), a protein kinase, (c) ether-a-go-go-related gene potassium channel 1 (HERG) (CHEMBL240), a voltage-gated ion channel, (d) Dopamine D4 receptor (CHEMBL219), a monoamine GPCR, (e) Coagulation factor X (CHEMBL244), a serine protease, (f) Cannabinoid CB1 receptor (CHEMBL218), a lipid-like GPCR and (g) Cytochrome P450 19A1 (CHEMBL1978), a cytochrome P450. The activity classes were selected based on data availability and as representatives of therapeutically important target classes or as anti-targets. . In this notebook we will create an fastai cnn learner on the same dataset as in the last notebook and then extract the second to last layer with a so called hook. We can then apply the activations from this layer to every image in the dataset. In the next notebook we will use these activations to hopefully better the performance of our Random Forest model. . dataset=&#39;CHEMBL205_cl&#39; . Generate activations . IMAGES=path/&#39;mol_images&#39;/f&#39;{dataset}&#39; . Create train validation split . def get_df(data): name = data.stem df = pd.read_csv(data) df[&#39;Image&#39;] = df[&#39;CID&#39;].apply(lambda x: f&#39;{str(x)}.png&#39;) df = df[[&#39;CID&#39;, &#39;SMILES&#39;, &#39;Image&#39;, &#39;Activity&#39;]] return df, name . x_train,x_val = train_test_split(df.index, test_size=0.25, stratify=df[&#39;Activity&#39;], random_state=42) df.loc[x_train, &#39;is_valid&#39;]=False df.loc[x_val, &#39;is_valid&#39;]=True . Create dataloader . df[&#39;Image&#39;] = df[&#39;CID&#39;] + &#39;.png&#39; df.head() . CID SMILES Activity is_valid Image . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc2)C=2c3c(OC4=CC(=O)C=CC=24)cc(O)cc3)ccc1 | 1 | True | CHEMBL188002.png | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | False | CHEMBL364127.png | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O)C2(C)C | 1 | False | CHEMBL1683469.png | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | False | CHEMBL52564.png | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | False | CHEMBL21427.png | . db = DataBlock( blocks = (ImageBlock(), CategoryBlock()), get_x=ColReader(&#39;Image&#39;, pref=IMAGES), get_y=ColReader(&#39;Activity&#39;), splitter=ColSplitter(&#39;is_valid&#39;), item_tfms=None, batch_tfms=None ) . dls = db.dataloaders(df, bs=64, shuffle_train=True) . dls.show_batch() . Train CNN model . def train_model(dls, arch=resnet18, epochs=6, freeze_epochs=5, wd=None): print(f&#39;Training model on dataset: {dataset}&#39;) print(f&#39;Architechture: {arch}&#39;) print(f&#39;Untrained epochs: freeze_epochs={freeze_epochs}&#39;) print(f&#39;Trained epochs: epochs={epochs}&#39;) learn = cnn_learner(dls, arch=resnet18, pretrained=True, wd=wd, metrics=[accuracy, F1Score(), Precision(), Recall(), RocAucBinary(), MatthewsCorrCoef()]) print(f&#39;Finding learning rate...&#39;) lr_min, lr_steep = learn.lr_find(suggestions=True, show_plot=False) print(f&#39;Training model with learning rate: {lr_min}&#39;) learn.fine_tune(epochs, lr_min, freeze_epochs=freeze_epochs) return learn . learn = train_model(dls) . Training model on dataset: CHEMBL205_cl Architechture: &lt;function resnet18 at 0x7fe49bb13b80&gt; Untrained epochs: freeze_epochs=5 Trained epochs: epochs=6 Finding learning rate... . Training model with learning rate: 0.02089296132326126 . epoch train_loss valid_loss accuracy f1_score precision_score recall_score roc_auc_score matthews_corrcoef time . 0 | 0.297757 | 0.182952 | 0.932234 | 0.600000 | 0.647727 | 0.558824 | 0.920301 | 0.565033 | 01:03 | . 1 | 0.206005 | 0.156396 | 0.948506 | 0.674189 | 0.794020 | 0.585784 | 0.944888 | 0.655752 | 01:03 | . 2 | 0.196921 | 0.130718 | 0.955194 | 0.733775 | 0.798271 | 0.678922 | 0.972067 | 0.712261 | 01:01 | . 3 | 0.158815 | 0.129715 | 0.962996 | 0.788265 | 0.821809 | 0.757353 | 0.977929 | 0.768798 | 01:03 | . 4 | 0.142770 | 0.114239 | 0.951404 | 0.686782 | 0.829861 | 0.585784 | 0.978075 | 0.673090 | 01:02 | . epoch train_loss valid_loss accuracy f1_score precision_score recall_score roc_auc_score matthews_corrcoef time . 0 | 0.128076 | 0.158720 | 0.948952 | 0.667634 | 0.818505 | 0.563725 | 0.944693 | 0.654097 | 01:22 | . 1 | 0.133582 | 0.115550 | 0.963888 | 0.817156 | 0.757322 | 0.887255 | 0.977314 | 0.800337 | 01:26 | . 2 | 0.117920 | 0.097336 | 0.963665 | 0.799508 | 0.802469 | 0.796569 | 0.978159 | 0.779537 | 01:20 | . 3 | 0.089624 | 0.108425 | 0.956531 | 0.720230 | 0.868512 | 0.615196 | 0.984091 | 0.709611 | 01:21 | . 4 | 0.052601 | 0.089789 | 0.969015 | 0.831515 | 0.822542 | 0.840686 | 0.985702 | 0.814515 | 01:20 | . 5 | 0.029035 | 0.105855 | 0.970129 | 0.835381 | 0.837438 | 0.833333 | 0.983640 | 0.818958 | 00:49 | . interp.plot_confusion_matrix() . interp.plot_top_losses(k=5) . Hook the activations from the second last layer . class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . hook_output = Hook() hook = learn.model[-1][-5].register_forward_hook(hook_output.hook_func) . test_db = DataBlock( blocks=(ImageBlock(), CategoryBlock()), get_x=ColReader(&#39;Image&#39;, pref=IMAGES), get_y=ColReader(&#39;Activity&#39;), splitter=RandomSplitter(valid_pct=0.), item_tfms=None, batch_tfms=None ) . test_dls = test_db.dataloaders(df_nodupl, bs=1) . test_dls.items.head() . CID SMILES Activity is_valid Image . 14250 CHEMBL239935 | [nH]1c2c3c(nnc2c2c1cccc2)cccc3 | 0 | False | CHEMBL239935.png | . 9720 CHEMBL1997503 | Clc1cc(ccc1OC(C)C)-c1nc(on1)-c1ccc(NC2CC(CC2)C(=O)[O-])cc1 | 0 | False | CHEMBL1997503.png | . 9468 CHEMBL257293 | Fc1ccc(cc1)CC1CCC[NH+](C1)C(O)CC(NC(=O)Nc1cc(cc(c1)C(C)C)-c1nnnn1C)C | 0 | False | CHEMBL257293.png | . 5423 CHEMBL478611 | O1CC(O)(Cc2cc(O)c(O)cc2)C(O)c2c1c(O)c(O)cc2 | 0 | False | CHEMBL478611.png | . 14108 CHEMBL111872 | n1c(Cc2ccccc2)c(n(C)c1N)Cc1ccccc1 | 0 | True | CHEMBL111872.png | . Defining the function for getting activations of a single image in the dataset . df_nodupl[&#39;activations&#39;] = None . def get_activations(CID, ret=False): idx = df_nodupl.loc[df_nodupl.CID == CID].index[0] print(idx) data = get_data(CID) img = data[0] with torch.no_grad(): output = learn.predict(img) acts = np.array(hook_output.stored.cpu())[0] #df_nodupl.iloc[idx][&#39;acts&#39;] = list(acts) if not ret: df_nodupl.at[idx, &#39;activations&#39;] = list(acts) if ret: return list(acts) . Getting 512 activations for every image in the dataframe and adding it as a single column . %%capture for CID in df_nodupl.CID.values: get_activations(CID) . Splitting each activation to its own column in the dataframe . df_acts = pd.concat([df_nodupl, pd.DataFrame(df_nodupl.activations.values.tolist()).add_prefix(&#39;act_&#39;)], axis=1) . df_acts.drop(&#39;activations&#39;, axis=1, inplace=True) . df_acts.head() . CID SMILES Activity is_valid Image act_0 act_1 act_2 act_3 act_4 ... act_502 act_503 act_504 act_505 act_506 act_507 act_508 act_509 act_510 act_511 . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc2)C=2c3c(OC4=CC(=O)C=CC=24)cc(O)cc3)ccc1 | 1 | True | CHEMBL188002.png | -42.897369 | 34.907032 | -35.413910 | -21.410141 | -61.686359 | ... | 98.090538 | -40.475765 | -55.758678 | -40.196342 | -21.565161 | -39.386112 | 88.342987 | -77.520561 | 30.948782 | -48.423111 | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | False | CHEMBL364127.png | -96.240280 | 17.625538 | 46.354603 | -167.131653 | -92.642540 | ... | 105.425598 | -53.179356 | -1.957846 | -152.047241 | -124.394402 | -114.585510 | 123.068222 | -48.347355 | 94.419861 | -130.973587 | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O)C2(C)C | 1 | False | CHEMBL1683469.png | -110.437485 | 1.140767 | 84.031761 | -205.667786 | -99.510849 | ... | 102.208374 | -64.332008 | 33.721348 | -192.948685 | -178.112000 | -143.599503 | 147.062531 | -47.969799 | 126.356018 | -158.886963 | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | False | CHEMBL52564.png | -42.642754 | 18.759075 | 5.093142 | -42.919945 | -57.405197 | ... | 105.915039 | -39.527615 | -23.374825 | -81.049446 | -53.409107 | -57.353622 | 98.793694 | -46.956039 | 50.408783 | -67.285126 | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | False | CHEMBL21427.png | -52.613388 | 29.321335 | -8.920407 | -38.327934 | -74.483788 | ... | 127.697937 | -41.359352 | -46.951527 | -80.075348 | -57.078045 | -62.758518 | 107.616997 | -67.371101 | 54.477600 | -71.601402 | . 5 rows × 517 columns . df_acts.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17941 entries, 0 to 17940 Columns: 517 entries, CID to act_511 dtypes: float64(512), int64(1), object(4) memory usage: 70.8+ MB . Add ECFP to dataframe . Also adding ECFP fingerprint to the dataframe and storing it as a separate csv. . df = df_ecfp.merge(df_acts, on=&#39;CID&#39;) . df.head() . CID SMILES_x Activity_x ECFP4_1 ECFP4_2 ECFP4_3 ECFP4_4 ECFP4_5 ECFP4_6 ECFP4_7 ... act_502 act_503 act_504 act_505 act_506 act_507 act_508 act_509 act_510 act_511 . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc2)C=2c3c(OC4=CC(=O)C=CC=24)cc(O)cc3)ccc1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 98.090538 | -40.475765 | -55.758678 | -40.196342 | -21.565161 | -39.386112 | 88.342987 | -77.520561 | 30.948782 | -48.423111 | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 105.425598 | -53.179356 | -1.957846 | -152.047241 | -124.394402 | -114.585510 | 123.068222 | -48.347355 | 94.419861 | -130.973587 | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O)C2(C)C | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 102.208374 | -64.332008 | 33.721348 | -192.948685 | -178.112000 | -143.599503 | 147.062531 | -47.969799 | 126.356018 | -158.886963 | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 105.915039 | -39.527615 | -23.374825 | -81.049446 | -53.409107 | -57.353622 | 98.793694 | -46.956039 | 50.408783 | -67.285126 | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 127.697937 | -41.359352 | -46.951527 | -80.075348 | -57.078045 | -62.758518 | 107.616997 | -67.371101 | 54.477600 | -71.601402 | . 5 rows × 1543 columns . df.drop([&#39;SMILES_y&#39;, &quot;Activity_y&quot;], axis=1, inplace=True) . df.head() . CID SMILES_x Activity_x ECFP4_1 ECFP4_2 ECFP4_3 ECFP4_4 ECFP4_5 ECFP4_6 ECFP4_7 ... act_502 act_503 act_504 act_505 act_506 act_507 act_508 act_509 act_510 act_511 . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc2)C=2c3c(OC4=CC(=O)C=CC=24)cc(O)cc3)ccc1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 98.090538 | -40.475765 | -55.758678 | -40.196342 | -21.565161 | -39.386112 | 88.342987 | -77.520561 | 30.948782 | -48.423111 | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 105.425598 | -53.179356 | -1.957846 | -152.047241 | -124.394402 | -114.585510 | 123.068222 | -48.347355 | 94.419861 | -130.973587 | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O)C2(C)C | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 102.208374 | -64.332008 | 33.721348 | -192.948685 | -178.112000 | -143.599503 | 147.062531 | -47.969799 | 126.356018 | -158.886963 | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 105.915039 | -39.527615 | -23.374825 | -81.049446 | -53.409107 | -57.353622 | 98.793694 | -46.956039 | 50.408783 | -67.285126 | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 127.697937 | -41.359352 | -46.951527 | -80.075348 | -57.078045 | -62.758518 | 107.616997 | -67.371101 | 54.477600 | -71.601402 | . 5 rows × 1541 columns . df.to_csv(path/&#39;chembl205-data-with-ecfp-activations.csv&#39;, index=None) .",
            "url": "https://drug-discovery.github.io/Drug-Discovery-blog/jupyter/activations/2021/04/24/3.0-create_activations.html",
            "relUrl": "/jupyter/activations/2021/04/24/3.0-create_activations.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "2 - Image Classification with DNN",
            "content": "DATASETS: . (a) Carbonic Anhydrase II (ChEMBL205), a protein lyase, (b) Cyclin-dependent kinase 2 (CHEMBL301), a protein kinase, (c) ether-a-go-go-related gene potassium channel 1 (HERG) (CHEMBL240), a voltage-gated ion channel, (d) Dopamine D4 receptor (CHEMBL219), a monoamine GPCR, (e) Coagulation factor X (CHEMBL244), a serine protease, (f) Cannabinoid CB1 receptor (CHEMBL218), a lipid-like GPCR and (g) Cytochrome P450 19A1 (CHEMBL1978), a cytochrome P450. The activity classes were selected based on data availability and as representatives of therapeutically important target classes or as anti-targets. . Generate images . DATA = path/&#39;mol_images&#39; DATA.mkdir(exist_ok=True) . df.head() . CID SMILES Activity . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc... | 1 | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O... | 1 | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | . Here we are creating images(.pngs) of the fingerprints from all of the molecules in the specific dataset by using functions from rdKit library. . IMAGES = DATA/dataset if not IMAGES.is_dir(): IMAGES.mkdir(exist_ok=True) for i, r in df.iterrows(): cid = r.CID smile = r.SMILES mol = Chem.MolFromSmiles(smile) Chem.Draw.MolToFile(mol, IMAGES/f&#39;{cid}.png&#39;, size = (224, 224), imageType=&#39;png&#39;) . images = list(IMAGES.glob(&#39;*.png&#39;)) . Plotting a specified fingerprint image of a molecule, image was created above. . Create train validation split . Splitting the dataset into a training and a validation set. . x_train, x_valid = train_test_split(df.index, test_size=0.2, random_state=666, stratify=df[&#39;Activity&#39;]) df.loc[x_train, &#39;is_valid&#39;]=False df.loc[x_valid, &#39;is_valid&#39;]=True . Create dataloader . Matching the CID with the .png and creating an extra column for extracting data. . df[&#39;Image&#39;] = df[&#39;CID&#39;] + &#39;.png&#39; df.head() . CID SMILES Activity is_valid Image . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc2)C=2c3c(OC4=CC(=O)C=CC=24)cc(O)cc3)ccc1 | 1 | False | CHEMBL188002.png | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | False | CHEMBL364127.png | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O)C2(C)C | 1 | False | CHEMBL1683469.png | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | False | CHEMBL52564.png | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | False | CHEMBL21427.png | . images = df[&#39;CID&#39;] + &quot;.png&quot; images.head() . 0 CHEMBL188002.png 1 CHEMBL364127.png 2 CHEMBL1683469.png 3 CHEMBL52564.png 4 CHEMBL21427.png Name: CID, dtype: object . db = DataBlock( blocks = (ImageBlock(), CategoryBlock()), get_x=ColReader(&#39;Image&#39;, pref=IMAGES), get_y=ColReader(&#39;Activity&#39;), splitter=ColSplitter(&#39;is_valid&#39;), item_tfms=None, batch_tfms=None ) . dls = db.dataloaders(df, bs=64, shuffle_train=True) . dls.show_batch() . Train CNN model . Creating a function for training the CNN model . def train_model(dls, arch=resnet18, epochs=5, freeze_epochs=5, wd=None): print(f&#39;Training model on dataset: {dataset}&#39;) print(f&#39;Architechture: {arch}&#39;) print(f&#39;Untrained epochs: freeze_epochs={freeze_epochs}&#39;) print(f&#39;Trained epochs: epochs={epochs}&#39;) learn = cnn_learner(dls, arch=resnet18, pretrained=True, wd=wd, metrics=[accuracy, F1Score(), Precision(), Recall(), RocAucBinary(), MatthewsCorrCoef()]) print(f&#39;Finding learning rate...&#39;) lr_min, lr_steep = learn.lr_find(suggestions=True, show_plot=False) print(f&#39;Training model with learning rate: {lr_min}&#39;) learn.fine_tune(epochs, lr_min, freeze_epochs=freeze_epochs) return learn . Training the model on a pretrained 18 layer convolutional neural network (resnet18) . learn = train_model(dls) . Training model on dataset: CHEMBL205_cl Architechture: &lt;function resnet18 at 0x7fdfe4fdfdc0&gt; Untrained epochs: freeze_epochs=5 Trained epochs: epochs=5 Finding learning rate... . Training model with learning rate: 0.012022644281387329 . epoch train_loss valid_loss accuracy f1_score precision_score recall_score roc_auc_score matthews_corrcoef time . 0 | 0.342263 | 0.171671 | 0.939537 | 0.643678 | 0.692580 | 0.601227 | 0.923221 | 0.612649 | 00:36 | . 1 | 0.202568 | 0.153371 | 0.942324 | 0.683969 | 0.680851 | 0.687117 | 0.947923 | 0.652245 | 00:35 | . 2 | 0.182909 | 0.179358 | 0.945389 | 0.620155 | 0.842105 | 0.490798 | 0.951055 | 0.618094 | 00:36 | . 3 | 0.160613 | 0.144208 | 0.958206 | 0.748322 | 0.825926 | 0.684049 | 0.968537 | 0.729586 | 00:37 | . 4 | 0.134017 | 0.121451 | 0.959877 | 0.766990 | 0.811644 | 0.726994 | 0.969901 | 0.746465 | 00:36 | . epoch train_loss valid_loss accuracy f1_score precision_score recall_score roc_auc_score matthews_corrcoef time . 0 | 0.115935 | 0.134479 | 0.960713 | 0.777953 | 0.799353 | 0.757669 | 0.967814 | 0.756746 | 00:46 | . 1 | 0.123517 | 0.126850 | 0.945667 | 0.606061 | 0.887574 | 0.460123 | 0.980248 | 0.616316 | 00:46 | . 2 | 0.092025 | 0.123103 | 0.963778 | 0.781145 | 0.865672 | 0.711656 | 0.966289 | 0.765950 | 00:46 | . 3 | 0.061702 | 0.095234 | 0.969908 | 0.837349 | 0.822485 | 0.852761 | 0.978670 | 0.820938 | 00:46 | . 4 | 0.024768 | 0.104723 | 0.972416 | 0.847458 | 0.851393 | 0.843558 | 0.979126 | 0.832306 | 00:46 | . # Train on resnet34 learn = train_model(dls, arch=resnet34) . Training model on dataset: CHEMBL205_cl Architechture: &lt;function resnet34 at 0x7fdfe4fdfe50&gt; Untrained epochs: freeze_epochs=5 Trained epochs: epochs=5 Finding learning rate... . Training model with learning rate: 0.010000000149011612 . epoch train_loss valid_loss accuracy f1_score precision_score recall_score roc_auc_score matthews_corrcoef time . 0 | 0.367343 | 0.187044 | 0.934522 | 0.611570 | 0.663082 | 0.567485 | 0.901495 | 0.578135 | 00:36 | . 1 | 0.205518 | 0.146825 | 0.941767 | 0.641509 | 0.727626 | 0.573620 | 0.946827 | 0.615416 | 00:35 | . 2 | 0.194592 | 0.156037 | 0.944274 | 0.699700 | 0.685294 | 0.714724 | 0.949034 | 0.669182 | 00:36 | . 3 | 0.173044 | 0.127663 | 0.954026 | 0.716007 | 0.815686 | 0.638037 | 0.967989 | 0.697579 | 00:37 | . 4 | 0.120203 | 0.116550 | 0.960435 | 0.768730 | 0.819444 | 0.723926 | 0.973296 | 0.748903 | 00:37 | . epoch train_loss valid_loss accuracy f1_score precision_score recall_score roc_auc_score matthews_corrcoef time . 0 | 0.135130 | 0.153385 | 0.938980 | 0.540881 | 0.854305 | 0.395706 | 0.957404 | 0.556781 | 00:46 | . 1 | 0.110067 | 0.119228 | 0.964057 | 0.806015 | 0.790560 | 0.822086 | 0.962950 | 0.786399 | 00:46 | . 2 | 0.098182 | 0.109664 | 0.958206 | 0.729242 | 0.885965 | 0.619632 | 0.981415 | 0.720657 | 00:46 | . 3 | 0.052791 | 0.094781 | 0.971023 | 0.839506 | 0.844720 | 0.834356 | 0.977985 | 0.823599 | 00:46 | . 4 | 0.020465 | 0.108049 | 0.972416 | 0.843602 | 0.869707 | 0.819018 | 0.978350 | 0.828940 | 00:46 | . . interp = ClassificationInterpretation.from_learner(learn) . Plotting the confusion matrix for visualising the performance of our classification model. . interp.plot_confusion_matrix() . Plotting the molecules/fingerprints with highest loss in our imageset. . interp.plot_top_losses(k=5) .",
            "url": "https://drug-discovery.github.io/Drug-Discovery-blog/jupyter/dnn/fingerprint/2021/04/24/2.0-image_classification.html",
            "relUrl": "/jupyter/dnn/fingerprint/2021/04/24/2.0-image_classification.html",
            "date": " • Apr 24, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "1 - Data preparations and Random Forest model",
            "content": "Backround . The article uses the highly used and preprocessed bioactivity dataset from the ChEMBL database, version 20. More specifically these seven different bioactivity classes: . (a) Carbonic Anhydrase II (ChEMBL205), a protein lyase, (b) Cyclin-dependent kinase 2 (CHEMBL301), a protein kinase, (c) ether-a-go-go-related gene potassium channel 1 (HERG) (CHEMBL240), a voltage-gated ion channel, (d) Dopamine D4 receptor (CHEMBL219), a monoamine GPCR, (e) Coagulation factor X (CHEMBL244), a serine protease, (f) Cannabinoid CB1 receptor (CHEMBL218), a lipid-like GPCR and (g) Cytochrome P450 19A1 (CHEMBL1978), a cytochrome P450. The activity classes were selected based on data availability and as representatives of therapeutically important target classes or as anti-targets. . In this project we will mainly focus on (a) the ChEMBL205 and (f) ChEMBL218 as these got quite different results from each other. Also . First we need to convert the smiles from the dataset to ECFP fingerprint which is hashed into 1024 length bits. For this we made a function for creating a fingerprint using the methods from the RDKit library as follows: . def fp(smile, diam = 2, bits = 1024): mol = Chem.MolFromSmiles(smile) Chem.SanitizeMol(mol) fp = AllChem.GetMorganFingerprintAsBitVect(mol, diam, nBits = bits) return fp . We then define a method for converting the csv with smiles to also have the fingerprint stored as colums from ECFP_1 to ECFP_1024 for each row. . #Generated Circular fingerprints hashed into n bits length vectors. def ECFP(ifile, ofile, diam, bits): print(f&quot;Making fingerprints for file: {ifile}&quot;) df = pd.read_csv(ifile) df.insert(2, &quot;ECFP4_&quot;, df.SMILES.apply(fp)) df[[f&quot;ECFP4_{i+1}&quot; for i in range(len(df.ECFP4_[0]))]] = df.ECFP4_.to_list() df.drop(&quot;ECFP4_&quot;, axis = 1, inplace = True) df.to_csv(path/ofile, index = None) return df . Here we can specify the dataset we want to use and to run the ECFP function on. . dataset=&#39;CHEMBL205_cl&#39; . #ECFP(path/f&#39;{dataset}.csv&#39;, f&#39;./{dataset}_ecfp_1024.csv&#39;, 2, 1024) . Next we create a dataframe from the newly generated csv file. . df = pd.read_csv(path/f&#39;{dataset}_ecfp_1024.csv&#39;) . df.head() . CID SMILES Activity ECFP4_1 ECFP4_2 ECFP4_3 ECFP4_4 ECFP4_5 ECFP4_6 ECFP4_7 ... ECFP4_1015 ECFP4_1016 ECFP4_1017 ECFP4_1018 ECFP4_1019 ECFP4_1020 ECFP4_1021 ECFP4_1022 ECFP4_1023 ECFP4_1024 . 0 CHEMBL188002 | S(=O)(=O)(N)c1cc(N/C(/S)=N c2cc(C(=O)[O-])c(cc... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 CHEMBL364127 | Clc1ccc(cc1)C(=O)NC1Cc2cc(S(=O)(=O)N)ccc2C1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 2 CHEMBL1683469 | S(=O)(=O)(N)c1ccc(cc1)CNS(=O)(=O)CC12CCC(CC1=O... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 3 CHEMBL52564 | Oc1ccccc1 C=C C(=O)[O-] | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 CHEMBL21427 | OB(O)c1ccc(OC)cc1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 rows × 1027 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17941 entries, 0 to 17940 Columns: 1027 entries, CID to ECFP4_1024 dtypes: int64(1025), object(2) memory usage: 140.6+ MB . X, y = df.drop([&quot;CID&quot;, &quot;SMILES&quot;, &quot;Activity&quot;], axis=1), df[&quot;Activity&quot;] . Train test split . X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=666) . 5-Fold Cross Validation . Follows the article where the data is split using 5-fold cross validation. . kf = KFold(n_splits=5, shuffle=True, random_state=999) . X_train_list, X_valid_list, y_train_list, y_valid_list = [], [], [], [] for train_index, valid_index in kf.split(X_train): X_train_list.append(X_train.iloc[train_index]) X_valid_list.append(X_train.iloc[valid_index]) y_train_list.append(y_train.iloc[train_index]) y_valid_list.append(y_train.iloc[valid_index]) . Random Forest comparison . Here, a Random Forest model is used, the settings from the paper is used with going through the different setting in each CV iteration. A Random Forest model is usually a very good performance model for classification, that is the main reason for its usage here. . param_grid = { &#39;n_estimators&#39;: [10,50,100,200,300,700], &#39;criterion&#39;: [&#39;gini&#39;, &#39;entropy&#39;], &#39;max_features&#39;: [&#39;log2&#39;, &#39;sqrt&#39;] } param_grid = ParameterGrid(param_grid) . Function for training a Random Forest classifier with the specified parameters, default: n_estimators=5, criterion=&#39;gini&#39;, max_features=&#39;log2&#39; . def train_rf(X_train, X_test, y_train, y_test, n_estimators=5, criterion=&#39;gini&#39;, max_features=&#39;log2&#39;): rf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, min_samples_split=2, max_features=max_features, max_leaf_nodes=None,bootstrap=False,oob_score=False, n_jobs=-1, random_state=69) rf.fit(X_train,y_train) y_pred= rf.predict(X_test) y_pred_prob=rf.predict_proba(X_test) temp=[] for j in range(len(y_pred_prob)): temp.append(y_pred_prob[j][1]) auc=roc_auc_score(np.array(y_test),np.array(temp)) acc2=accuracy_score(y_test,y_pred) mcc=matthews_corrcoef(y_test,y_pred) Recall=recall_score(y_test, y_pred,pos_label=1) Precision=precision_score(y_test, y_pred,pos_label=1) F1_score=f1_score(y_test, y_pred,pos_label=1) return auc,acc2,mcc,Recall,Precision,F1_score . Next we test all the different settings on the random forest model . for setting in param_grid: print(f&quot;Testing combination: {setting}&quot;) aucs, accs, mccs, recalls, precs, f1_scores = [], [], [], [], [], [] for i in range(0,5): X_train = X_train_list[i] X_valid = X_valid_list[i] y_train = y_train_list[i] y_valid = y_valid_list[i] auc,acc2,mcc,recall,precision,F1_score = train_rf(X_train, X_test, y_train, y_test, n_estimators=setting[&#39;n_estimators&#39;], criterion=setting[&#39;criterion&#39;], max_features=setting[&#39;max_features&#39;]) aucs.append(auc) accs.append(acc2) mccs.append(mcc) recalls.append(recall) precs.append(precision) f1_scores.append(F1_score) print(f&quot;Average ROCAUC of the folds: {np.mean(aucs)}&quot;) print(f&quot;Average accuracy of the folds: {np.mean(accs)}&quot;) print(f&quot;Average Matthews correlation of the folds: {np.mean(mccs)}&quot;) print(f&quot;Average recall of the folds: {np.mean(recalls)}&quot;) print(f&quot;Average precision of the folds: {np.mean(precs)}&quot;) print(f&quot;Average f1 score of the folds: {np.mean(f1_scores)}&quot;) print() . Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 10} Average ROCAUC of the folds: 0.9772882110994567 Average accuracy of the folds: 0.9667270447262087 Average Matthews correlation of the folds: 0.7860500038292063 Average recall of the folds: 0.7372307692307694 Average precision of the folds: 0.8760069960166821 Average f1 score of the folds: 0.800503358063245 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 50} Average ROCAUC of the folds: 0.9883235554088932 Average accuracy of the folds: 0.9716316009474711 Average Matthews correlation of the folds: 0.8209589646616985 Average recall of the folds: 0.7926153846153847 Average precision of the folds: 0.8822409748917097 Average f1 score of the folds: 0.83500841879966 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 100} Average ROCAUC of the folds: 0.9897701382423307 Average accuracy of the folds: 0.971743068134318 Average Matthews correlation of the folds: 0.8215468433293995 Average recall of the folds: 0.7923076923076924 Average precision of the folds: 0.883681362094854 Average f1 score of the folds: 0.8354713493040207 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 200} Average ROCAUC of the folds: 0.9903762124194175 Average accuracy of the folds: 0.9720774696948584 Average Matthews correlation of the folds: 0.8238100152984001 Average recall of the folds: 0.7953846153846154 Average precision of the folds: 0.8846597606510033 Average f1 score of the folds: 0.8376368086620165 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 300} Average ROCAUC of the folds: 0.9905535114494821 Average accuracy of the folds: 0.9722168036784172 Average Matthews correlation of the folds: 0.8249672684508293 Average recall of the folds: 0.7984615384615384 Average precision of the folds: 0.8835781774076714 Average f1 score of the folds: 0.838854526090383 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 700} Average ROCAUC of the folds: 0.9908532604212089 Average accuracy of the folds: 0.9721889368817054 Average Matthews correlation of the folds: 0.8248672775803916 Average recall of the folds: 0.7990769230769231 Average precision of the folds: 0.8827487938727472 Average f1 score of the folds: 0.8388064256004263 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 10} Average ROCAUC of the folds: 0.9772568148872729 Average accuracy of the folds: 0.9685941201058939 Average Matthews correlation of the folds: 0.801694981605307 Average recall of the folds: 0.7766153846153846 Average precision of the folds: 0.8629193877802063 Average f1 score of the folds: 0.8174913388674341 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 50} Average ROCAUC of the folds: 0.9871096392499794 Average accuracy of the folds: 0.9709627978263897 Average Matthews correlation of the folds: 0.8197484029807942 Average recall of the folds: 0.812923076923077 Average precision of the folds: 0.858933897696472 Average f1 score of the folds: 0.8352657808393531 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 100} Average ROCAUC of the folds: 0.9887458721759319 Average accuracy of the folds: 0.9715758673540476 Average Matthews correlation of the folds: 0.8238137649341255 Average recall of the folds: 0.818153846153846 Average precision of the folds: 0.8610968655939681 Average f1 score of the folds: 0.8390530575198107 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 200} Average ROCAUC of the folds: 0.9888152879753923 Average accuracy of the folds: 0.9723840044586873 Average Matthews correlation of the folds: 0.8289251494410698 Average recall of the folds: 0.8233846153846155 Average precision of the folds: 0.8651762364626471 Average f1 score of the folds: 0.8437446620112299 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 300} Average ROCAUC of the folds: 0.9891161447714228 Average accuracy of the folds: 0.9723561376619756 Average Matthews correlation of the folds: 0.828566538703412 Average recall of the folds: 0.821846153846154 Average precision of the folds: 0.8660686746587011 Average f1 score of the folds: 0.843364299633399 Testing combination: {&#39;criterion&#39;: &#39;gini&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 700} Average ROCAUC of the folds: 0.9894152101919836 Average accuracy of the folds: 0.9724397380521109 Average Matthews correlation of the folds: 0.829156614702144 Average recall of the folds: 0.8227692307692308 Average precision of the folds: 0.8662129059726817 Average f1 score of the folds: 0.8439271211097632 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 10} Average ROCAUC of the folds: 0.9804304486688429 Average accuracy of the folds: 0.9679253169848125 Average Matthews correlation of the folds: 0.7937533455572263 Average recall of the folds: 0.7427692307692307 Average precision of the folds: 0.8846853049526121 Average f1 score of the folds: 0.8074645155199276 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 50} Average ROCAUC of the folds: 0.9900928922464084 Average accuracy of the folds: 0.971854535321165 Average Matthews correlation of the folds: 0.822328710871904 Average recall of the folds: 0.7935384615384615 Average precision of the folds: 0.8838282161189858 Average f1 score of the folds: 0.8362435918877612 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 100} Average ROCAUC of the folds: 0.9905116969746969 Average accuracy of the folds: 0.9718266685244531 Average Matthews correlation of the folds: 0.8221385357433695 Average recall of the folds: 0.7932307692307692 Average precision of the folds: 0.8838186927317839 Average f1 score of the folds: 0.8360435636998712 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 200} Average ROCAUC of the folds: 0.9910789501596916 Average accuracy of the folds: 0.9724118712553992 Average Matthews correlation of the folds: 0.8262529644111316 Average recall of the folds: 0.8 Average precision of the folds: 0.8843894471999943 Average f1 score of the folds: 0.8400419434408519 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 300} Average ROCAUC of the folds: 0.9911039115626217 Average accuracy of the folds: 0.9723561376619758 Average Matthews correlation of the folds: 0.8261580994481952 Average recall of the folds: 0.8018461538461539 Average precision of the folds: 0.8822698543953127 Average f1 score of the folds: 0.8400967620207005 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;log2&#39;, &#39;n_estimators&#39;: 700} Average ROCAUC of the folds: 0.9911414361645707 Average accuracy of the folds: 0.9723840044586876 Average Matthews correlation of the folds: 0.8264496010081249 Average recall of the folds: 0.803076923076923 Average precision of the folds: 0.8815263060754923 Average f1 score of the folds: 0.8404267804418 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 10} Average ROCAUC of the folds: 0.9792251593970607 Average accuracy of the folds: 0.9692350564302632 Average Matthews correlation of the folds: 0.8064290801915908 Average recall of the folds: 0.7849230769230768 Average precision of the folds: 0.8631094931906522 Average f1 score of the folds: 0.8221043342025652 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 50} Average ROCAUC of the folds: 0.9888097959953329 Average accuracy of the folds: 0.9719938693047233 Average Matthews correlation of the folds: 0.826327019013666 Average recall of the folds: 0.8196923076923076 Average precision of the folds: 0.8641589152909088 Average f1 score of the folds: 0.841309602980508 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 100} Average ROCAUC of the folds: 0.9898620405180847 Average accuracy of the folds: 0.9724118712553992 Average Matthews correlation of the folds: 0.8292029075535048 Average recall of the folds: 0.8243076923076924 Average precision of the folds: 0.8647413354240697 Average f1 score of the folds: 0.8440388779058366 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 200} Average ROCAUC of the folds: 0.9899568655643423 Average accuracy of the folds: 0.9726348056290929 Average Matthews correlation of the folds: 0.8306298858661861 Average recall of the folds: 0.8258461538461539 Average precision of the folds: 0.865803297784208 Average f1 score of the folds: 0.8453519472560641 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 300} Average ROCAUC of the folds: 0.9901267869559582 Average accuracy of the folds: 0.9724118712553992 Average Matthews correlation of the folds: 0.8293221619482496 Average recall of the folds: 0.8252307692307692 Average precision of the folds: 0.8640346330095527 Average f1 score of the folds: 0.8441788146153677 Testing combination: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;n_estimators&#39;: 700} Average ROCAUC of the folds: 0.9903818693945858 Average accuracy of the folds: 0.9726626724258047 Average Matthews correlation of the folds: 0.8309066881967601 Average recall of the folds: 0.8267692307692307 Average precision of the folds: 0.8653842324791043 Average f1 score of the folds: 0.8456313021672045 . Next, we choose the settings that gave us the best score and train a the model on these on all the folds. The results for each cross validation is displayed and the MCC compared to the best average score from the paper. . MCC=[] . # Train and print scores for each split for i in range(0,5): print(f&#39;Cross validation iteration: {i + 1}&#39;) X_train = X_train_list[i] X_valid = X_valid_list[i] y_train = y_train_list[i] y_valid = y_valid_list[i] auc,acc2,mcc,Recall,Precision,F1_score = train_rf(X_train, X_valid, y_train, y_valid, n_estimators=200, criterion=&#39;entropy&#39;, max_features=&#39;sqrt&#39;) MCC.append(mcc) print(f&#39;AUC score:{auc}&#39;) print(f&#39;Accuracy: {acc2}&#39;) print(f&#39;Matthews: {mcc}&#39;) print(f&#39;Recall: {Recall}&#39;) print(f&#39;Precision: {Precision}&#39;) print(f&#39;F1 score: {F1_score}&#39;) print() . Cross validation iteration: 1 AUC score:0.9895547345611034 Accuracy: 0.9716674407803065 Matthews: 0.8322885854488427 Recall: 0.8415841584158416 Precision: 0.8542713567839196 F1 score: 0.8478802992518704 Cross validation iteration: 2 AUC score:0.9881408694710627 Accuracy: 0.9772410589874594 Matthews: 0.8543605077494866 Recall: 0.8457446808510638 Precision: 0.888268156424581 F1 score: 0.8664850136239782 Cross validation iteration: 3 AUC score:0.99115142437103 Accuracy: 0.972131908964236 Matthews: 0.8182680063069008 Recall: 0.819672131147541 Precision: 0.847457627118644 F1 score: 0.8333333333333333 Cross validation iteration: 4 AUC score:0.9761998796790828 Accuracy: 0.9684161634928008 Matthews: 0.8252479636345342 Recall: 0.7964601769911505 Precision: 0.8910891089108911 F1 score: 0.8411214953271028 Cross validation iteration: 5 AUC score:0.992038545211134 Accuracy: 0.9763011152416357 Matthews: 0.8437545323349981 Recall: 0.8351648351648352 Precision: 0.8786127167630058 F1 score: 0.8563380281690139 . . np.mean(MCC), top_mcc_scores[&#39;CHEMBL205&#39;] . (0.8360720667107868, 0.862) .",
            "url": "https://drug-discovery.github.io/Drug-Discovery-blog/2021/04/24/1.0-Data_and_RF.html",
            "relUrl": "/2021/04/24/1.0-Data_and_RF.html",
            "date": " • Apr 24, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://drug-discovery.github.io/Drug-Discovery-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}